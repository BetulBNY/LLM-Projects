{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c74dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf891b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI() # a new instance of the OpenAI Python Client library, a lightweight wrapper around making HTTP calls to an endpoint for calling the GPT LLM, or other LLM providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d200b",
   "metadata": {},
   "source": [
    "#### A Message to OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3863a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Betül!\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6865084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Betül! Nice to meet you. How can I assist you today?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366474ea",
   "metadata": {},
   "source": [
    "#### Follow-up Question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b3b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de069d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m not sure what your name is. Could you please tell me?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e1bb4",
   "metadata": {},
   "source": [
    "#### LLMs Themselves Have No Permanent Memory\n",
    "\n",
    "A base language model (like GPT-4, Claude, Gemini, etc.) is stateless.\n",
    "That means:\n",
    "* It does not “remember” anything from past conversations once the session ends.\n",
    "* It processes only the current input tokens (your message + recent history).\n",
    "* When the chat is closed, that data is gone.\n",
    "\n",
    "\n",
    "So the model’s parameters (the “weights”) are fixed — they don’t change while chatting.\n",
    "It’s not like a human brain that stores new experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4886265f",
   "metadata": {},
   "source": [
    "#### Then How Does It “Remember” During a Conversation?\n",
    "\n",
    "During an ongoing chat (one session), everything me and Chat say is part of a context window.\n",
    "It is like a temporary short-term memory.\n",
    "The model doesn’t “store” your name permanently — it just reads it again from the chat history still in the _context window_.\n",
    "That window might hold tens of thousands of tokens (for GPT-4-turbo, up to ~128k tokens).\n",
    "Once the context gets too long or the chat resets, that information disappears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a5f3f",
   "metadata": {},
   "source": [
    "When you use ChatGPT (the app), OpenAI adds a “memory” feature:\n",
    "\n",
    "* Certain facts you share (like your department, projects, or interests) can be stored by the system.\n",
    "\n",
    "* When you return in a new chat, that info is automatically loaded into Chat's context.\n",
    "\n",
    "* The model itself didn’t remember — the platform did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82319bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Betül!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Betül! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "532de1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Betül! How can I help you today, Betül?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d593a",
   "metadata": {},
   "source": [
    "1. Every call to an LLM is stateless\n",
    "2. We pass in the entire conversation so far in the input prompt, every time\n",
    "3. This gives the illusion that the LLM has memory - it apparently keeps the context of the conversation\n",
    "4. But this is a trick; it's a by-product of providing the entire conversation, every time\n",
    "5. An LLM just predicts the most likely next tokens in the sequence; if that sequence contains \"My name is Betül\" and later \"What's my name?\" then it will predict.. Betül!\n",
    "\n",
    "The ChatGPT product uses exactly this trick - every time you send a message, it's the entire conversation that gets passed in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
